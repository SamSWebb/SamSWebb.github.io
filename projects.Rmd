---
title: "Projects"
---

Here you can find my current projects, where I am leading author, all are open access, or otherwise stated. Hyperlinks take you direct to pre-prints, publications, and data repositories.

![](Project%20collage.png){width="100%"} 

## <span style="color:black">**Pending publications**

## <span style="color:black">**Stage 1 Accepted Registered Reports:**

### <span style="color:grey90">[Pre-Attentive Processing in Visuospatial Neglect: Burning Houses Revisited](https://osf.io/5trcx/)

-   The project seeks to replicate the pre-attentive semantic processing effect first documented by [Marshall and Halligan (1988)](https://www.nature.com/articles/336766a0) under more stringent experimental conditions, and in a broad, representative sample while investigating whether semantic or merely physical stimuli differences are responsible for modulating the occurrence of pre-attentive processing in neglect patients.

## <span style="color:black">**Preprints:**

### <span style="color:grey90">[Using multiverse analysis to highlight differences in convergent correlation outcomes due to data analytical and study design choices](https://osf.io/cktj9/)

In neuropsychological research there are a near limitless number of different approaches researchers can choose when designing studies. Here we showcase the multiverse technique to establish the robustness of analytical pathways choices within classic psychometric test validation in an example test of executive function. 
We examined the impact of choices regarding sample groups, sample sizes, test metrics, and covariate inclusions on convergent validation correlations between tests of executive function. Data were available for 87 neurologically healthy adults and 117 stroke survivors, and a total of 2,220 different analyses were run in a multiverse analysis. 
We found that the type of sample group, sample size, and test metric used for analyses impacted validation outcomes. Covariate inclusion choices did not impact the observed coefficients in our analyses.  
The present analysis demonstrates the importance of carefully justifying every aspect of a psychometric test validation study a priori with theoretical and statistical factors in mind. It is essential to thoroughly consider the purpose and use of a new tool when designing validation studies. 

-   Pre-print available on [PsyArxiv](https://psyarxiv.com/nhrwq/)

- Status: preparing to submit for publication

## <span style="color:black">**Ongoing Projects 2022-:**

### <span style="color:grey90">The Oxford Cognitive Screen: Validation of a 20-minute remote version ##

The recent upatake of teleneuropsychology is expanding the collection of clinicaly useful standardised and validated tools for remote administration. We pre-registered an evaluation of performance across face-to-face and remote (i.e., video or telephone conferencing) administrations of paper-based the Oxford Cognitive Screen (OCS) in community based stroke survivors.
We used two recruitment streams from two longitudinal stroke cohorts (i.e., OCS-RECOVERY and OX-CHRONIC studies - see www.ocs-test.org) based in Oxfordshire, United Kingdom where participants were either approximately 6 months or 4 years post-stroke at assessment date. Participants were able to choose phone or video conferencing for administration of the OCS. Order of remote or face-to-face administration was counterbalanced. We used repeated measures t-tests and intraclass correlation coefficients to compare performance between modalities.
31 stroke survivors were recruited; age M=70.29(SD=11.19); education M=13.84(SD=2.98), sex = 32.26% F, OCS-Recovery days since stroke M=211.29 (SD=48.67); OX-CHRONIC years since stroke M=5.44 (SD=2.1). All completed the OCS both in person and remotely on average M=40.87, (SD=54.43) days apart. We found that performance across both modalities were very similar and most of those who had domain impairments in one version had the same impairments in the other modality. There was, however, a general trend in number of domains impaired, to be more impaired on the remote version of the OCS. The OCS can be run both in person and remotely via the phone or video call, with specific tasks being sent ahead of time via post. 

- Status: preparing to submit for publication / pre-print

## <span style="color:black">**Ongoing Projects 2021-:**

### <span style="color:grey90">Ecological and predictive validity of the Oxford Digital Multiple Errands Test (OxMET) in stroke ##

The Oxford Digital Multiple Errands Test (OxMET) is a brief cognitive screen, intended as an ecologically valid approach to assessing complex executive function, in contrast to traditional abstract neuropsychological assessments with high language demands. Initial psychometric validation of the OxMET found high internal reliability and test-retest reliability, as well as good convergent and divergent validity. Here we assess aspects of predictive and ecological validity of the OxMET measures in relation to commonly used clinical function scales in a subacute stroke sample.
Participants within 3 months of confirmed stroke were recruited from a UK English-speaking specialist stroke rehabilitation in-patient setting between 2020 and 2022. Each completed the OxMET with a researcher, whilst clinical outcome measures were collected from medical notes: Barthel Index and stroke the specific Therapy Outcome Measure scale.
As of March 2022: Participants (n=333) were on average 24.94 days post stroke (SD=16.46), had a mean age of 73.8 (SD =13.05) and mean NIH Stroke Severity of 8.24 (SD= 5.32). We followed up a subset of 48 participants to complete activities of daily living and cognitive failures questionnaires to compare to baseline OxMET performance.
At baseline we found a significant but small relationship between the OxMET accuracy Barthel Index (r=.22) but we did not find a relationship between functioning and more specialist OxMET metrics such as error types. At 6-month follow we found a moderate relationship between the OxMET accuracy metric and activities of daily living (R2=.31)
The OxMET, an executive function assessment screen, was found to associate to specialist stroke relevant measures of functionality both in a rehabilitation context and community setting, but not beyond accuracy on the task. We note that the measures have a strong physical focus which may limit associations. The OxMET could be a useful and ecologically relevant tool in assessing executive function in those with limited physical or language abilities.

-   Status: Under active recruitment

### <span style="color:grey90">Validation of the Oxford Cognitive Screen-Plus in UK-based English-speaking subacute and chronic stroke survivor cohorts

Stroke survivors are routinely screened for cognitive impairment with screening tools which often fail to detect subtle impairments. The Oxford Cognitive Screen â€“ Plus (OCS-Plus) is a brief computer tablet-based screening tool designed to detect more subtle impairments. We aimed to examine its psychometric properties in two stroke cohorts (subacute = <3 months post-stroke and chronic = 6 months post-stroke).
As of March 2022: 338 stroke survivors (172 subacute, 166 chronic, average age 73 (SD=13), education 13 (SD=3), sex 42.73% female presenting, ~3.42% ischaemic stroke), from two in-patient stroke units in the UK were recruited between May 2016 and March 2022, and a subset was followed up at 6-months post-stroke. All participants completed the OCS and OCS-Plus. A subset additionally completed the Montreal Cognitive Assessment (MoCA), and further format- and construct-matched neuropsychological tests. Due to the tablet-based format, OCS-Plus provides standardised assessment for non-specialist administration and automatic real time impairment classification against a matched normative set.
Construct validity criteria were met and exceeded the psychometric findings in the original psychometric validation using healthy control data. Incidence of cognitive impairment was higher in subacute stroke survivors compared to chronic stroke survivors, with the highest rates of impairment found for visuospatial and executive subtests. We report the sensitivity and specificity of each OCS-Plus subtest compared to neuropsychological test performance. Notably, many of those who were unimpaired on the MoCA and Oxford Cognitive Screen, demonstrated impairment on neuropsychological testing and OCS-Plus screening.
The OCS-Plus is a valid cognitive screening tool for use in stroke. OCS-Plus is able to detect subtle cognitive impairment at a similar level to selected neuropsychological assessments and exceeds detection of impairment compared to the OCS and MoCA.

-   Status: Under active recruitment

### <span style="color:grey90">[The Oxford COMPetency ASSessment(COMPASS): A Brief Supplemental Cognitive Assessment Aligned with Mental Capacity Criteria](http://www.demeyerelab.org/?page_id=469)

Assessment of mental capacity is a critical aspect of clinical practice. Mental capacity is a legal concept defined in statute by the Mental Capacity Act 2005 (England and Wales). Current best practice consists of a qualitative interview to elucidate the ability to (i) understand, (ii) retain and (iii) weigh up the specific information at hand, and to (iv) communicate a decision. Capacity assessments often fail to align with the legal standards and are often contentious with low agreement rates. We developed and validated a new brief neuropsychological screening tool (The Oxford Competency Assessment; COMPASS) to assess the cognitive constructs aligned to the core abilities required for mental capacity. 
122 neurologically healthy participants were compared with 117 participants with neurological conditions (stroke or dementia/mild cognitive impairment) on COMPASS performance. The validation included 56 control participants and 69 neurological participants who completed additional neuropsychological tasks including the MoCA. 29 participants were re-tested. 80 participants were compared against a mental capacity assessment on setting up a hypothetical Lasting Power of Attorney.
We found great reliability of the COMPASS, convergent validity analysis revealed low but significant correlations, and divergent validation was achieved. The COMPASS total score was similar to the MoCA in identifying impairment in mental capacity assessments, but neither were adequate to differentiate impairment in core abilities (i.e., understanding, retention, or weighing up)
Further research comparing the COMPASS to capacity assessments with differing complexity is necessary, and to see whether the COMPASS increases interrater reliability in capacity assessments.
- Status: In preparation


## <span style="color:black">**Unpublished / Unpre-printed works**

### [MSc Dissertation](https://osf.io/cktj9/)

-   This forms the storage for my MSc work

-   Including

    -   Final anonymised data (both controls and patients data)
    -   Analysis script to reproduce my analysis
    -   Original pre-registration and files explaining changes along the research pipeline

### [Essay on alternative facts in psychology](https://osf.io/5wmkr/)

-   This was an essay I wrote in 2017 for my second year of my undergraduate when I just got into Open science. I uploaded this more as a personal joke as I had no idea about open science at that point and had only just stumbled into the state of psychology (i.e., it is on fire).

