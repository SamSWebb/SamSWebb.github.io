---
title: "Projects"
---

Here you can find my current projects, where I am leading author, all are open access, or otherwise stated. Hyperlinks take you direct to pre-prints, publications, and data repositories.

![](Project%20collage.png){width="100%"} 

## <span style="color:black">**Pending publications**
...

## <span style="color:black">**Stage 1 Accepted Registered Reports:**

### <span style="color:grey90">[Pre-Attentive Processing in Visuospatial Neglect: Burning Houses Revisited](https://osf.io/5trcx/)

-   The project seeks to replicate the pre-attentive semantic processing effect first documented by [Marshall and Halligan (1988)](https://www.nature.com/articles/336766a0) under more stringent experimental conditions, and in a broad, representative sample while investigating whether semantic or merely physical stimuli differences are responsible for modulating the occurrence of pre-attentive processing in neglect patients.

## <span style="color:black">**Preprints:**

### <span style="color:grey90">[Multiverse Analysis: A Method to Determine Researcher Degrees of Freedom in Test Validation](https://osf.io/cktj9/)

In psychometric validation there are a near limitless number of different approaches researchers can decide to take when choosing sample type and size, test metrics, and covariate measures. It is particularly important to factor in these different analytical pathways when validating tools intended for use in clinical populations. Here, we aimed to provide a tutorial paper showcasing the multiverse, or specification curve, technique to establish the robustness of analytical pathways on psychometric validation outcomes in an example test of executive function. We examined the impact of choices regarding sample groups, sample sizes, different types of metrics from a given test, covariate inclusions, and outlier removal on convergent validation correlations between new tests of executive function. Data were available for 88 neurologically healthy adults and 117 stroke survivors, a total of 6,660 different analyses were run. We found that the type of sample group and sample size used for analyses impacted validation outcomes. Comparing test metrics within-category (e.g.accuracy vs accuracy) rather than between-category (e.g., accuracy vs time) significantly increased the observed correlation coefficients. Covariate inclusion and outlier removal choices did not impact the observed coefficients in our analyses. This tutorial paper showcases how authors can use the multiverse technique to evidence the robustness, or fragility, of their validation outcomes across many different analytical pathways. The example here highlights the importance of fully justifying and considering the analytical pathway to take in validation work on clinically relevant tools.

-   Pre-print available on [PsyArxiv](https://psyarxiv.com/nhrwq/)

- Status: under revision

## <span style="color:black">**Ongoing Projects-:**

### <span style="color:grey90">Ecological and predictive validity of the Oxford Digital Multiple Errands Test (OxMET) in stroke ##

The Oxford Digital Multiple Errands Test (OxMET) is a brief cognitive screen, intended as an ecologically valid approach to assessing complex executive function, in contrast to traditional abstract neuropsychological assessments with high language demands. Initial psychometric validation of the OxMET found high internal reliability and test-retest reliability, as well as good convergent and divergent validity. For my DPhil I am further investigating the ecological validity of the OxMET in three phases:  

1.    Assessing the relation of the OxMET to commonly used clinical function scales in a subacute stroke sample.
Participants within 3 months of confirmed stroke are being recruited from a UK English-speaking specialist stroke rehabilitation in-patient setting. Each completes the OxMET with a researcher, whilst clinical outcome measures are collected from medical notes: Barthel Index, the Modified Rankin scale, and the stroke specific Therapy Outcome Measure scale.
2.    Assessing whether OxMET, when administered in the rehab unit, predicts activities of daily living and/or subjective cognitive complains at a 6-month follow up from phase 1. Participants recruited in phase 1 are being followed up and administered the Cognitive Failures Questionnaire and ADL measures, as well as estimates of modified rankin scale. 
3.    In phase 3, we are actively recruiting neurologically healthy controls and stroke survivors (at any time post-stroke) to complete the OxMET as well as the [MET-Home](www.met-home.com). We aim to compare the feasibility of administration of the OxMET and MET-Home in the community setting as well as correlate the OxMET and MET-Home together and further assess the relationship between ADLs in the community and OxMET performance. 


### <span style="color:grey90">[The Oxford COMPetency ASSessment(COMPASS): A Brief Supplemental Cognitive Assessment Aligned with Mental Capacity Criteria](http://www.demeyerelab.org/?page_id=469)

Assessment of mental capacity is a critical aspect of clinical practice. Mental capacity is a legal concept defined in statute by the Mental Capacity Act 2005 (England and Wales). Current best practice consists of a qualitative interview to elucidate the ability to (i) understand, (ii) retain and (iii) weigh up the specific information at hand, and to (iv) communicate a decision. Capacity assessments often fail to align with the legal standards and are often contentious with low agreement rates. We developed and validated a new brief neuropsychological screening tool (The Oxford Competency Assessment; COMPASS) to assess the cognitive constructs aligned to the core abilities required for mental capacity. 
122 neurologically healthy participants were compared with 117 participants with neurological conditions (stroke or dementia/mild cognitive impairment) on COMPASS performance. The validation included 56 control participants and 69 neurological participants who completed additional neuropsychological tasks including the MoCA. 29 participants were re-tested. 80 participants were compared against a mental capacity assessment on setting up a hypothetical Lasting Power of Attorney.
We found great reliability of the COMPASS, convergent validity analysis revealed low but significant correlations, and divergent validation was achieved. The COMPASS total score was similar to the MoCA in identifying impairment in mental capacity assessments, but neither were adequate to differentiate impairment in core abilities (i.e., understanding, retention, or weighing up)
Further research comparing the COMPASS to capacity assessments with differing complexity is necessary, and to see whether the COMPASS increases interrater reliability in capacity assessments.
- Status: In preparation


## <span style="color:black">**Unpublished / Unpre-printed works**

### [MSc Dissertation](https://osf.io/cktj9/)

-   This forms the storage for my MSc work

-   Including

    -   Final anonymised data (both controls and patients data)
    -   Analysis script to reproduce my analysis
    -   Original pre-registration and files explaining changes along the research pipeline

### [Essay on alternative facts in psychology](https://osf.io/5wmkr/)

-   This was an essay I wrote in 2017 for my second year of my undergraduate when I just got into Open science. I uploaded this more as a personal joke as I had no idea about open science at that point and had only just stumbled into the state of psychology (i.e., it is on fire).

