---
title: "Projects"
---

Here you can find my current projects, where I am leading author, all are open access, or otherwise stated. Hyperlinks take you direct to pre-prints, publications, and data repositories.

![](Project%20collage.png){width="100%"} 

## <span style="color:black">**Pending publications**

-

## <span style="color:black">**Stage 1 Accepted Registered Reports:**

### <span style="color:grey90">[Pre-Attentive Processing in Visuospatial Neglect: Burning Houses Revisited](https://osf.io/5trcx/)

-   The project seeks to replicate the pre-attentive semantic processing effect first documented by [Marshall and Halligan (1988)](https://www.nature.com/articles/336766a0) under more stringent experimental conditions, and in a broad, representative sample while investigating whether semantic or merely physical stimuli differences are responsible for modulating the occurrence of pre-attentive processing in neglect patients.

## <span style="color:black">**Preprints:**

### <span style="color:grey90">[Multiverse Analysis: A Method to Determine Researcher Degrees of Freedom in Test Validation](https://osf.io/cktj9/)

In psychometric validation there are a near limitless number of different approaches researchers can decide to take when choosing sample type and size, test metrics, and covariate measures. It is particularly important to factor in these different analytical pathways when validating tools intended for use in clinical populations. Here, we aimed to provide a tutorial paper showcasing the multiverse, or specification curve, technique to establish the robustness of analytical pathways on psychometric validation outcomes in an example test of executive function. We examined the impact of choices regarding sample groups, sample sizes, different types of metrics from a given test, covariate inclusions, and outlier removal on convergent validation correlations between new tests of executive function. Data were available for 88 neurologically healthy adults and 117 stroke survivors, a total of 6,660 different analyses were run. We found that the type of sample group and sample size used for analyses impacted validation outcomes. Comparing test metrics within-category (e.g.accuracy vs accuracy) rather than between-category (e.g., accuracy vs time) significantly increased the observed correlation coefficients. Covariate inclusion and outlier removal choices did not impact the observed coefficients in our analyses. This tutorial paper showcases how authors can use the multiverse technique to evidence the robustness, or fragility, of their validation outcomes across many different analytical pathways. The example here highlights the importance of fully justifying and considering the analytical pathway to take in validation work on clinically relevant tools.

-   Pre-print available on [PsyArxiv](https://psyarxiv.com/nhrwq/)

- Status: under revision

### <span style="color:grey90">[Validation of the UK English Oxford Cognitive Screen-Plus in Sub-Acute and Chronic Stroke Survivors.](https://osf.io/t8zug/)

Stroke survivors are routinely screened for cognitive impairment with tools that often fail to detect subtle impairments. The Oxford Cognitive Screen-Plus (OCS-Plus) is a brief tablet-based screen designed  to  detect  subtle  post-stroke  cognitive  impairments.  We  examined  its  psychometric properties  in  two  stroke  cohorts  (subacute:  <3  months  post-stroke,  chronic:  >6  months  post-stroke). This study included 347 stroke survivors(meanage = 73years; mean education = 13 years; 43.06% female  presenting;  74.42%  ischaemic  stroke). The OCS-Plus  was  completed  by  181sub-acute stroke  survivors  and  166  chronic  stroke  survivors. All  participants also completed  the  Oxford Cognitive Screen (OCS)and a subset completed the Montreal Cognitive Assessment (MoCA) and further neuropsychological tests. First,  convergent  construct  validity  of  all  OCS-Plus  tasks  against  task-matched  standardized neuropsychological tests was confirmed. Second, we evaluated divergent construct validity of all OCS-Plus subtasks. Third,  we  report  the  sensitivity  and  specificity  of  each  OCS-Plus subtask compared  to  neuropsychological  test  performance.  Fourth, we found  that OCS-Plus  detected cognitive impairments in a large proportion of those classed as unimpaired on MoCA and OCS. The OCS-Plus provides  a valid screening  tool for  sensitive  detection  of  subtle  cognitive impairment  in  stroke  patients. Indeed,  the  OCS-Plus  detected  subtle  cognitive  impairment  at  a similar  level  to  validated neuropsychological  assessments  and  exceeded  detection  of cognitive impairment compared to standard clinicals creening tools.

-   Pre-print available on [PsyArxiv](https://psyarxiv.com/y8we5/)

- Status: under revision



## <span style="color:black">**Ongoing Projects 2021-:**

### <span style="color:grey90">Ecological and predictive validity of the Oxford Digital Multiple Errands Test (OxMET) in stroke ##

The Oxford Digital Multiple Errands Test (OxMET) is a brief cognitive screen, intended as an ecologically valid approach to assessing complex executive function, in contrast to traditional abstract neuropsychological assessments with high language demands. Initial psychometric validation of the OxMET found high internal reliability and test-retest reliability, as well as good convergent and divergent validity. Here we assess aspects of predictive and ecological validity of the OxMET measures in relation to commonly used clinical function scales in a subacute stroke sample.
Participants within 3 months of confirmed stroke were recruited from a UK English-speaking specialist stroke rehabilitation in-patient setting between 2020 and 2022. Each completed the OxMET with a researcher, whilst clinical outcome measures were collected from medical notes: Barthel Index and stroke the specific Therapy Outcome Measure scale.
As of March 2022: Participants (n=333) were on average 24.94 days post stroke (SD=16.46), had a mean age of 73.8 (SD =13.05) and mean NIH Stroke Severity of 8.24 (SD= 5.32). We followed up a subset of 48 participants to complete activities of daily living and cognitive failures questionnaires to compare to baseline OxMET performance.
At baseline we found a significant but small relationship between the OxMET accuracy Barthel Index (r=.22) but we did not find a relationship between functioning and more specialist OxMET metrics such as error types. At 6-month follow we found a moderate relationship between the OxMET accuracy metric and activities of daily living (R2=.31)
The OxMET, an executive function assessment screen, was found to associate to specialist stroke relevant measures of functionality both in a rehabilitation context and community setting, but not beyond accuracy on the task. We note that the measures have a strong physical focus which may limit associations. The OxMET could be a useful and ecologically relevant tool in assessing executive function in those with limited physical or language abilities.

-   Status: Under active recruitment

### <span style="color:grey90">[The Oxford COMPetency ASSessment(COMPASS): A Brief Supplemental Cognitive Assessment Aligned with Mental Capacity Criteria](http://www.demeyerelab.org/?page_id=469)

Assessment of mental capacity is a critical aspect of clinical practice. Mental capacity is a legal concept defined in statute by the Mental Capacity Act 2005 (England and Wales). Current best practice consists of a qualitative interview to elucidate the ability to (i) understand, (ii) retain and (iii) weigh up the specific information at hand, and to (iv) communicate a decision. Capacity assessments often fail to align with the legal standards and are often contentious with low agreement rates. We developed and validated a new brief neuropsychological screening tool (The Oxford Competency Assessment; COMPASS) to assess the cognitive constructs aligned to the core abilities required for mental capacity. 
122 neurologically healthy participants were compared with 117 participants with neurological conditions (stroke or dementia/mild cognitive impairment) on COMPASS performance. The validation included 56 control participants and 69 neurological participants who completed additional neuropsychological tasks including the MoCA. 29 participants were re-tested. 80 participants were compared against a mental capacity assessment on setting up a hypothetical Lasting Power of Attorney.
We found great reliability of the COMPASS, convergent validity analysis revealed low but significant correlations, and divergent validation was achieved. The COMPASS total score was similar to the MoCA in identifying impairment in mental capacity assessments, but neither were adequate to differentiate impairment in core abilities (i.e., understanding, retention, or weighing up)
Further research comparing the COMPASS to capacity assessments with differing complexity is necessary, and to see whether the COMPASS increases interrater reliability in capacity assessments.
- Status: In preparation


## <span style="color:black">**Unpublished / Unpre-printed works**

### [MSc Dissertation](https://osf.io/cktj9/)

-   This forms the storage for my MSc work

-   Including

    -   Final anonymised data (both controls and patients data)
    -   Analysis script to reproduce my analysis
    -   Original pre-registration and files explaining changes along the research pipeline

### [Essay on alternative facts in psychology](https://osf.io/5wmkr/)

-   This was an essay I wrote in 2017 for my second year of my undergraduate when I just got into Open science. I uploaded this more as a personal joke as I had no idea about open science at that point and had only just stumbled into the state of psychology (i.e., it is on fire).

